{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DistilBert-Squadv1",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h-nV5KRIMWV"
      },
      "source": [
        "<img src=\"https://huggingface.co/front/assets/huggingface_logo.svg\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07KTcIzZ9avB"
      },
      "source": [
        "## Training DistilBert on SQUAD V-1\r\n",
        "\r\n",
        "In this Notebook, we will train DistilBert from [Huggingface](https://huggingface.co/transformers/v2.10.0/model_doc/distilbert.html). DistilBERT is a smaller version of bert and has faster inference and computation time as compare to standard BERT (base-uncased). The different variants of DistilBert are present [here](https://huggingface.co/transformers/pretrained_models.html)\r\n",
        "\r\n",
        "The [Squad Dataset](https://huggingface.co/datasets/squad) is taken from Huggingface Datasets and is trained with the DistilBERT base-cased for 0.75 iterations , and the pytorch model is preserved. The trained model can be found [here](https://huggingface.co/abhilash1910/distilbert-squadv1).\r\n",
        "\r\n",
        "\r\n",
        "The steps for using this model is:\r\n",
        "\r\n",
        "```python\r\n",
        "\r\n",
        "from transformers import AutoModelForQuestionAnswering,AutoTokenizer,pipeline\r\n",
        "model=AutoModelForQuestionAnswering.from_pretrained('abhilash1910/distilbert-squadv1')\r\n",
        "tokenizer=AutoTokenizer.from_pretrained('abhilash1910/distilbert-squadv1')\r\n",
        "nlp_QA=pipeline('question-answering',model=model,tokenizer=tokenizer)\r\n",
        "QA_inp={\r\n",
        "    'question': 'What is the fund price of Huggingface in NYSE?',\r\n",
        "    'context': 'Huggingface Co. has a total fund price of $19.6 million dollars'\r\n",
        "}\r\n",
        "result=nlp_QA(QA_inp)\r\n",
        "result\r\n",
        "```\r\n",
        "\r\n",
        "The result is:\r\n",
        "\r\n",
        "```bash\r\n",
        "\r\n",
        "{'score': 0.38547369837760925,\r\n",
        " 'start': 42,\r\n",
        " 'end': 55,\r\n",
        " 'answer': '$19.6 million'}\r\n",
        " ```\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx0NqbLjA4NC"
      },
      "source": [
        "## Importing libraries\r\n",
        "\r\n",
        "In this case we will be importing the necessary libraries, including [Datasets](https://pypi.org/project/datasets/). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHHgbj2JHRbM"
      },
      "source": [
        "import torch\r\n",
        "import logging\r\n",
        "import os\r\n",
        "import math\r\n",
        "import copy\r\n",
        "from dataclasses import dataclass, field\r\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2T4zqZAGH4_x",
        "outputId": "5d3c242e-71ed-4f52-cd3a-1a472e92499d"
      },
      "source": [
        "!pip install datasets"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/38/0c24dce24767386123d528d27109024220db0e7a04467b658d587695241a/datasets-1.1.3-py3-none-any.whl (153kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 14.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.23.0)\n",
            "Collecting pyarrow>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e1/27958a70848f8f7089bff8d6ebe42519daf01f976d28b481e1bfd52c8097/pyarrow-2.0.0-cp36-cp36m-manylinux2014_x86_64.whl (17.7MB)\n",
            "\u001b[K     |████████████████████████████████| 17.7MB 199kB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets) (0.8)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets) (1.1.5)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/73/826b19f3594756cb1c6c23d2fbd8ca6a77a9cd3b650c9dec5acc85004c38/xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 42.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets) (1.18.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: pyarrow, xxhash, datasets\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "Successfully installed datasets-1.1.3 pyarrow-2.0.0 xxhash-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJd4iW2YCKa_"
      },
      "source": [
        "## Downloading Transformers Locally\r\n",
        "\r\n",
        "In this case, we have to download [Transformers](https://github.com/huggingface/transformers/issues/8551) locally. And then navigate to the root of the directory. This allows us to use all the different classes for training (for different downstream tasks, MLM,NER,POS,Classification,QA,MNLI etc.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyUVZ0KGHtam"
      },
      "source": [
        "%%capture\r\n",
        "!git clone https://github.com/huggingface/transformers\r\n",
        "%cd transformers\r\n",
        "!pip install .\r\n",
        "!pip install -r ./examples/requirements.txt\r\n",
        "%cd ..\r\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsRJk6g7Crzq"
      },
      "source": [
        "## Python Scripts For Question Answering\r\n",
        "\r\n",
        "For training on Squad, this [repository](https://github.com/huggingface/transformers/tree/master/examples/question-answering) contains all the details. For this case, we require the 'run_qa.py','trainer_qa.py' and 'utils_qa.py' files, and we locally download them using '!wget'.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROJxqMq4HkTx",
        "outputId": "287476db-9a2c-4301-d5c4-229ca553b86c"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/question-answering/run_qa.py"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-16 16:17:45--  https://raw.githubusercontent.com/huggingface/transformers/master/examples/question-answering/run_qa.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21709 (21K) [text/plain]\n",
            "Saving to: ‘run_qa.py’\n",
            "\n",
            "\rrun_qa.py             0%[                    ]       0  --.-KB/s               \rrun_qa.py           100%[===================>]  21.20K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-12-16 16:17:45 (136 MB/s) - ‘run_qa.py’ saved [21709/21709]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FV5JT4qnHlZp",
        "outputId": "b405fa0e-5293-4b31-ae91-24d1d19f1a89"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/question-answering/trainer_qa.py"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-16 16:17:46--  https://raw.githubusercontent.com/huggingface/transformers/master/examples/question-answering/trainer_qa.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4539 (4.4K) [text/plain]\n",
            "Saving to: ‘trainer_qa.py’\n",
            "\n",
            "\rtrainer_qa.py         0%[                    ]       0  --.-KB/s               \rtrainer_qa.py       100%[===================>]   4.43K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-12-16 16:17:46 (78.9 MB/s) - ‘trainer_qa.py’ saved [4539/4539]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJdWTfm-Hn30",
        "outputId": "54c0c4a2-fc3c-466f-d267-c2652ddf2b6d"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/question-answering/utils_qa.py"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-16 16:17:49--  https://raw.githubusercontent.com/huggingface/transformers/master/examples/question-answering/utils_qa.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22242 (22K) [text/plain]\n",
            "Saving to: ‘utils_qa.py’\n",
            "\n",
            "\rutils_qa.py           0%[                    ]       0  --.-KB/s               \rutils_qa.py         100%[===================>]  21.72K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-12-16 16:17:49 (72.2 MB/s) - ‘utils_qa.py’ saved [22242/22242]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_YIkvswIDEm"
      },
      "source": [
        "OUTPUT_DIR='abhilash1910/distilbert-squadv1'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "305ekNAvD2fU"
      },
      "source": [
        "## Training using DistilBERT\r\n",
        "\r\n",
        "Here we have used the following parameters:\r\n",
        "\r\n",
        "- Training Batch Size : 512\r\n",
        "- Learning Rate : 3e-5\r\n",
        "- Training Epochs : 0.75\r\n",
        "- Sequence Length : 384\r\n",
        "- Stride : 128\r\n",
        "\r\n",
        "After training is completed, we save it locally in our colab. After this we can either download these locally to our machine and upload them using Git in Huggingface or we can directly upload the model from here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beIHJFJfHqNK",
        "outputId": "9653b326-7ce9-476a-c21d-d0ddbc71cf84"
      },
      "source": [
        "!python run_qa.py \\\r\n",
        "  --model_name_or_path 'distilbert-base-cased' \\\r\n",
        "  --dataset_name squad \\\r\n",
        "  --do_train \\\r\n",
        "  --do_eval \\\r\n",
        "  --per_device_train_batch_size 12 \\\r\n",
        "  --learning_rate 3e-5 \\\r\n",
        "  --num_train_epochs 0.75 \\\r\n",
        "  --max_seq_length 384 \\\r\n",
        "  --doc_stride 128 \\\r\n",
        "  --output_dir $OUTPUT_DIR/"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-16 16:17:53.373936: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "12/16/2020 16:17:55 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "12/16/2020 16:17:55 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='abhilash1910/distilbert-squadv1/', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, model_parallel=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=12, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=3e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.75, max_steps=-1, warmup_steps=0, logging_dir='runs/Dec16_16-17-55_44dd3baf367e', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='abhilash1910/distilbert-squadv1/', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fp16_backend='auto')\n",
            "Downloading: 5.24kB [00:00, 5.44MB/s]       \n",
            "Downloading: 2.19kB [00:00, 2.84MB/s]     \n",
            "Downloading and preparing dataset squad/plain_text (download: 33.51 MiB, generated: 85.75 MiB, post-processed: Unknown size, total: 119.27 MiB) to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41...\n",
            "Downloading: 30.3MB [00:00, 87.2MB/s]\n",
            "Downloading: 4.85MB [00:00, 87.7MB/s]       \n",
            "Dataset squad downloaded and prepared to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41. Subsequent calls will reuse this data.\n",
            "https://huggingface.co/distilbert-base-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp3ug7tga3\n",
            "Downloading: 100% 411/411 [00:00<00:00, 595kB/s]\n",
            "storing https://huggingface.co/distilbert-base-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
            "creating metadata file for /root/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
            "loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
            "Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
            "Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "https://huggingface.co/bert-base-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpsoyihmzh\n",
            "Downloading: 100% 213k/213k [00:00<00:00, 17.1MB/s]\n",
            "storing https://huggingface.co/bert-base-cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "creating metadata file for /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpwsrangsg\n",
            "Downloading: 100% 436k/436k [00:00<00:00, 17.0MB/s]\n",
            "storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
            "creating metadata file for /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
            "loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
            "https://huggingface.co/distilbert-base-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpj386xdzo\n",
            "Downloading: 100% 263M/263M [00:03<00:00, 71.8MB/s]\n",
            "storing https://huggingface.co/distilbert-base-cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19\n",
            "creating metadata file for /root/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19\n",
            "loading weights file https://huggingface.co/distilbert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19\n",
            "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 88/88 [00:44<00:00,  2.00ba/s]\n",
            "100% 11/11 [00:15<00:00,  1.37s/ba]\n",
            "Downloading: 4.02kB [00:00, 4.25MB/s]       \n",
            "Downloading: 3.35kB [00:00, 2.92MB/s]       \n",
            "The following columns in the training set don't have a corresponding argument in `DistilBertForQuestionAnswering.forward` and have been ignored: .\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
            "***** Running training *****\n",
            "  Num examples = 88729\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 12\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 5547\n",
            "{'loss': 2.973889404296875, 'learning_rate': 2.7295835586803676e-05, 'epoch': 0.0676132521974307}\n",
            "  9% 500/5547 [03:52<39:52,  2.11it/s]Saving model checkpoint to abhilash1910/distilbert-squadv1/checkpoint-500\n",
            "Configuration saved in abhilash1910/distilbert-squadv1/checkpoint-500/config.json\n",
            "Model weights saved in abhilash1910/distilbert-squadv1/checkpoint-500/pytorch_model.bin\n",
            "{'loss': 2.0022333984375, 'learning_rate': 2.4591671173607356e-05, 'epoch': 0.1352265043948614}\n",
            " 18% 1000/5547 [07:54<36:12,  2.09it/s]Saving model checkpoint to abhilash1910/distilbert-squadv1/checkpoint-1000\n",
            "Configuration saved in abhilash1910/distilbert-squadv1/checkpoint-1000/config.json\n",
            "Model weights saved in abhilash1910/distilbert-squadv1/checkpoint-1000/pytorch_model.bin\n",
            "{'loss': 1.7144573974609374, 'learning_rate': 2.188750676041103e-05, 'epoch': 0.2028397565922921}\n",
            " 27% 1500/5547 [11:56<32:05,  2.10it/s]Saving model checkpoint to abhilash1910/distilbert-squadv1/checkpoint-1500\n",
            "Configuration saved in abhilash1910/distilbert-squadv1/checkpoint-1500/config.json\n",
            "Model weights saved in abhilash1910/distilbert-squadv1/checkpoint-1500/pytorch_model.bin\n",
            "{'loss': 1.5969427490234376, 'learning_rate': 1.918334234721471e-05, 'epoch': 0.2704530087897228}\n",
            " 36% 2000/5547 [15:58<28:14,  2.09it/s]Saving model checkpoint to abhilash1910/distilbert-squadv1/checkpoint-2000\n",
            "Configuration saved in abhilash1910/distilbert-squadv1/checkpoint-2000/config.json\n",
            "Model weights saved in abhilash1910/distilbert-squadv1/checkpoint-2000/pytorch_model.bin\n",
            "{'loss': 1.4956407470703126, 'learning_rate': 1.647917793401839e-05, 'epoch': 0.3380662609871535}\n",
            " 45% 2500/5547 [20:00<24:21,  2.08it/s]Saving model checkpoint to abhilash1910/distilbert-squadv1/checkpoint-2500\n",
            "Configuration saved in abhilash1910/distilbert-squadv1/checkpoint-2500/config.json\n",
            "Model weights saved in abhilash1910/distilbert-squadv1/checkpoint-2500/pytorch_model.bin\n",
            "{'loss': 1.401275390625, 'learning_rate': 1.3775013520822065e-05, 'epoch': 0.4056795131845842}\n",
            " 54% 3000/5547 [24:02<20:26,  2.08it/s]Saving model checkpoint to abhilash1910/distilbert-squadv1/checkpoint-3000\n",
            "Configuration saved in abhilash1910/distilbert-squadv1/checkpoint-3000/config.json\n",
            "Model weights saved in abhilash1910/distilbert-squadv1/checkpoint-3000/pytorch_model.bin\n",
            "{'loss': 1.408527099609375, 'learning_rate': 1.1070849107625744e-05, 'epoch': 0.47329276538201487}\n",
            " 63% 3500/5547 [28:04<16:21,  2.09it/s]Saving model checkpoint to abhilash1910/distilbert-squadv1/checkpoint-3500\n",
            "Configuration saved in abhilash1910/distilbert-squadv1/checkpoint-3500/config.json\n",
            "Model weights saved in abhilash1910/distilbert-squadv1/checkpoint-3500/pytorch_model.bin\n",
            "{'loss': 1.345153076171875, 'learning_rate': 8.366684694429422e-06, 'epoch': 0.5409060175794456}\n",
            " 72% 4000/5547 [32:11<12:31,  2.06it/s]Saving model checkpoint to abhilash1910/distilbert-squadv1/checkpoint-4000\n",
            "Configuration saved in abhilash1910/distilbert-squadv1/checkpoint-4000/config.json\n",
            "Model weights saved in abhilash1910/distilbert-squadv1/checkpoint-4000/pytorch_model.bin\n",
            "{'loss': 1.3180711669921874, 'learning_rate': 5.662520281233099e-06, 'epoch': 0.6085192697768763}\n",
            " 81% 4500/5547 [36:13<08:20,  2.09it/s]Saving model checkpoint to abhilash1910/distilbert-squadv1/checkpoint-4500\n",
            "Configuration saved in abhilash1910/distilbert-squadv1/checkpoint-4500/config.json\n",
            "Model weights saved in abhilash1910/distilbert-squadv1/checkpoint-4500/pytorch_model.bin\n",
            "{'loss': 1.3147288818359375, 'learning_rate': 2.9583558680367764e-06, 'epoch': 0.676132521974307}\n",
            " 90% 5000/5547 [40:14<04:21,  2.09it/s]Saving model checkpoint to abhilash1910/distilbert-squadv1/checkpoint-5000\n",
            "Configuration saved in abhilash1910/distilbert-squadv1/checkpoint-5000/config.json\n",
            "Model weights saved in abhilash1910/distilbert-squadv1/checkpoint-5000/pytorch_model.bin\n",
            "{'loss': 1.2497662353515624, 'learning_rate': 2.541914548404543e-07, 'epoch': 0.7437457741717377}\n",
            " 99% 5500/5547 [44:16<00:22,  2.09it/s]Saving model checkpoint to abhilash1910/distilbert-squadv1/checkpoint-5500\n",
            "Configuration saved in abhilash1910/distilbert-squadv1/checkpoint-5500/config.json\n",
            "Model weights saved in abhilash1910/distilbert-squadv1/checkpoint-5500/pytorch_model.bin\n",
            "100% 5547/5547 [44:41<00:00,  2.09it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'epoch': 0.7501014198782961}\n",
            "100% 5547/5547 [44:41<00:00,  2.07it/s]\n",
            "Saving model checkpoint to abhilash1910/distilbert-squadv1/\n",
            "Configuration saved in abhilash1910/distilbert-squadv1/config.json\n",
            "Model weights saved in abhilash1910/distilbert-squadv1/pytorch_model.bin\n",
            "12/16/2020 17:04:05 - INFO - __main__ -   *** Evaluate ***\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10822\n",
            "  Batch size = 8\n",
            "100% 1353/1353 [02:30<00:00,  8.98it/s]12/16/2020 17:06:43 - INFO - utils_qa -   Post-processing 10570 example predictions split into 10822 features.\n",
            "\n",
            "  0% 0/10570 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 53/10570 [00:00<00:20, 524.31it/s]\u001b[A\n",
            "  1% 121/10570 [00:00<00:18, 561.53it/s]\u001b[A\n",
            "  2% 185/10570 [00:00<00:17, 581.09it/s]\u001b[A\n",
            "  2% 234/10570 [00:00<00:19, 543.63it/s]\u001b[A\n",
            "  3% 277/10570 [00:00<00:22, 457.48it/s]\u001b[A\n",
            "  3% 339/10570 [00:00<00:20, 495.38it/s]\u001b[A\n",
            "  4% 399/10570 [00:00<00:19, 522.48it/s]\u001b[A\n",
            "  4% 459/10570 [00:00<00:18, 542.46it/s]\u001b[A\n",
            "  5% 519/10570 [00:00<00:18, 557.33it/s]\u001b[A\n",
            "  5% 579/10570 [00:01<00:17, 569.45it/s]\u001b[A\n",
            "  6% 640/10570 [00:01<00:17, 578.59it/s]\u001b[A\n",
            "  7% 704/10570 [00:01<00:16, 593.25it/s]\u001b[A\n",
            "  7% 764/10570 [00:01<00:16, 585.56it/s]\u001b[A\n",
            "  8% 823/10570 [00:01<00:17, 543.75it/s]\u001b[A\n",
            "  8% 880/10570 [00:01<00:17, 551.18it/s]\u001b[A\n",
            "  9% 936/10570 [00:01<00:17, 538.91it/s]\u001b[A\n",
            "  9% 991/10570 [00:01<00:17, 535.79it/s]\u001b[A\n",
            " 10% 1045/10570 [00:01<00:17, 530.32it/s]\u001b[A\n",
            " 10% 1099/10570 [00:01<00:17, 530.08it/s]\u001b[A\n",
            " 11% 1158/10570 [00:02<00:17, 545.95it/s]\u001b[A\n",
            " 12% 1221/10570 [00:02<00:16, 566.62it/s]\u001b[A\n",
            " 12% 1282/10570 [00:02<00:16, 577.15it/s]\u001b[A\n",
            " 13% 1344/10570 [00:02<00:15, 587.06it/s]\u001b[A\n",
            " 13% 1403/10570 [00:02<00:15, 578.56it/s]\u001b[A\n",
            " 14% 1462/10570 [00:02<00:15, 577.72it/s]\u001b[A\n",
            " 14% 1524/10570 [00:02<00:15, 588.69it/s]\u001b[A\n",
            " 15% 1585/10570 [00:02<00:15, 593.33it/s]\u001b[A\n",
            " 16% 1645/10570 [00:02<00:15, 587.52it/s]\u001b[A\n",
            " 16% 1707/10570 [00:03<00:14, 596.19it/s]\u001b[A\n",
            " 17% 1768/10570 [00:03<00:14, 599.71it/s]\u001b[A\n",
            " 17% 1829/10570 [00:03<00:14, 600.69it/s]\u001b[A\n",
            " 18% 1892/10570 [00:03<00:14, 607.63it/s]\u001b[A\n",
            " 18% 1954/10570 [00:03<00:14, 609.27it/s]\u001b[A\n",
            " 19% 2015/10570 [00:03<00:14, 608.30it/s]\u001b[A\n",
            " 20% 2078/10570 [00:03<00:13, 613.57it/s]\u001b[A\n",
            " 20% 2140/10570 [00:03<00:14, 574.64it/s]\u001b[A\n",
            " 21% 2199/10570 [00:03<00:14, 578.53it/s]\u001b[A\n",
            " 21% 2258/10570 [00:03<00:14, 577.10it/s]\u001b[A\n",
            " 22% 2316/10570 [00:04<00:14, 577.56it/s]\u001b[A\n",
            " 22% 2374/10570 [00:04<00:14, 569.87it/s]\u001b[A\n",
            " 23% 2432/10570 [00:04<00:14, 564.96it/s]\u001b[A\n",
            " 24% 2489/10570 [00:04<00:14, 559.37it/s]\u001b[A\n",
            " 24% 2546/10570 [00:04<00:14, 538.01it/s]\u001b[A\n",
            " 25% 2602/10570 [00:04<00:14, 543.71it/s]\u001b[A\n",
            " 25% 2666/10570 [00:04<00:13, 568.96it/s]\u001b[A\n",
            " 26% 2724/10570 [00:04<00:13, 568.48it/s]\u001b[A\n",
            " 26% 2790/10570 [00:04<00:13, 591.16it/s]\u001b[A\n",
            " 27% 2850/10570 [00:04<00:13, 591.62it/s]\u001b[A\n",
            " 28% 2910/10570 [00:05<00:13, 584.28it/s]\u001b[A\n",
            " 28% 2970/10570 [00:05<00:12, 588.19it/s]\u001b[A\n",
            " 29% 3030/10570 [00:05<00:12, 590.52it/s]\u001b[A\n",
            " 29% 3090/10570 [00:05<00:12, 587.24it/s]\u001b[A\n",
            " 30% 3149/10570 [00:05<00:12, 575.28it/s]\u001b[A\n",
            " 30% 3207/10570 [00:05<00:12, 571.59it/s]\u001b[A\n",
            " 31% 3265/10570 [00:05<00:12, 569.57it/s]\u001b[A\n",
            " 31% 3323/10570 [00:05<00:12, 570.05it/s]\u001b[A\n",
            " 32% 3382/10570 [00:05<00:12, 573.61it/s]\u001b[A\n",
            " 33% 3440/10570 [00:06<00:12, 558.52it/s]\u001b[A\n",
            " 33% 3496/10570 [00:06<00:12, 556.79it/s]\u001b[A\n",
            " 34% 3552/10570 [00:06<00:12, 556.95it/s]\u001b[A\n",
            " 34% 3608/10570 [00:06<00:12, 557.82it/s]\u001b[A\n",
            " 35% 3665/10570 [00:06<00:12, 561.07it/s]\u001b[A\n",
            " 35% 3725/10570 [00:06<00:12, 569.94it/s]\u001b[A\n",
            " 36% 3783/10570 [00:06<00:11, 571.88it/s]\u001b[A\n",
            " 36% 3841/10570 [00:06<00:11, 565.96it/s]\u001b[A\n",
            " 37% 3898/10570 [00:06<00:11, 557.62it/s]\u001b[A\n",
            " 37% 3955/10570 [00:06<00:11, 557.60it/s]\u001b[A\n",
            " 38% 4011/10570 [00:07<00:12, 541.38it/s]\u001b[A\n",
            " 38% 4067/10570 [00:07<00:11, 544.43it/s]\u001b[A\n",
            " 39% 4122/10570 [00:07<00:12, 518.83it/s]\u001b[A\n",
            " 39% 4175/10570 [00:07<00:14, 444.51it/s]\u001b[A\n",
            " 40% 4222/10570 [00:07<00:15, 402.68it/s]\u001b[A\n",
            " 40% 4265/10570 [00:07<00:15, 398.59it/s]\u001b[A\n",
            " 41% 4307/10570 [00:07<00:19, 322.03it/s]\u001b[A\n",
            " 41% 4358/10570 [00:07<00:17, 361.25it/s]\u001b[A\n",
            " 42% 4415/10570 [00:08<00:15, 405.03it/s]\u001b[A\n",
            " 42% 4474/10570 [00:08<00:13, 446.97it/s]\u001b[A\n",
            " 43% 4525/10570 [00:08<00:13, 463.62it/s]\u001b[A\n",
            " 43% 4580/10570 [00:08<00:12, 485.85it/s]\u001b[A\n",
            " 44% 4632/10570 [00:08<00:12, 490.61it/s]\u001b[A\n",
            " 44% 4683/10570 [00:08<00:12, 485.83it/s]\u001b[A\n",
            " 45% 4743/10570 [00:08<00:11, 514.55it/s]\u001b[A\n",
            " 45% 4796/10570 [00:08<00:11, 491.91it/s]\u001b[A\n",
            " 46% 4852/10570 [00:08<00:11, 508.79it/s]\u001b[A\n",
            " 46% 4904/10570 [00:09<00:11, 499.50it/s]\u001b[A\n",
            " 47% 4962/10570 [00:09<00:10, 520.02it/s]\u001b[A\n",
            " 47% 5015/10570 [00:09<00:10, 521.99it/s]\u001b[A\n",
            " 48% 5070/10570 [00:09<00:10, 529.27it/s]\u001b[A\n",
            " 48% 5126/10570 [00:09<00:10, 537.93it/s]\u001b[A\n",
            " 49% 5182/10570 [00:09<00:09, 541.95it/s]\u001b[A\n",
            " 50% 5240/10570 [00:09<00:09, 550.07it/s]\u001b[A\n",
            " 50% 5296/10570 [00:09<00:09, 543.91it/s]\u001b[A\n",
            " 51% 5354/10570 [00:09<00:09, 552.14it/s]\u001b[A\n",
            " 51% 5411/10570 [00:09<00:09, 554.65it/s]\u001b[A\n",
            " 52% 5467/10570 [00:10<00:09, 520.15it/s]\u001b[A\n",
            " 52% 5520/10570 [00:10<00:09, 507.86it/s]\u001b[A\n",
            " 53% 5574/10570 [00:10<00:09, 516.59it/s]\u001b[A\n",
            " 53% 5628/10570 [00:10<00:09, 521.13it/s]\u001b[A\n",
            " 54% 5681/10570 [00:10<00:09, 517.79it/s]\u001b[A\n",
            " 54% 5737/10570 [00:10<00:09, 527.76it/s]\u001b[A\n",
            " 55% 5791/10570 [00:10<00:09, 529.00it/s]\u001b[A\n",
            " 55% 5846/10570 [00:10<00:08, 533.41it/s]\u001b[A\n",
            " 56% 5904/10570 [00:10<00:08, 545.07it/s]\u001b[A\n",
            " 56% 5962/10570 [00:10<00:08, 554.02it/s]\u001b[A\n",
            " 57% 6018/10570 [00:11<00:08, 543.10it/s]\u001b[A\n",
            " 57% 6073/10570 [00:11<00:08, 539.11it/s]\u001b[A\n",
            " 58% 6129/10570 [00:11<00:08, 543.03it/s]\u001b[A\n",
            " 59% 6186/10570 [00:11<00:07, 550.60it/s]\u001b[A\n",
            " 59% 6242/10570 [00:11<00:08, 515.97it/s]\u001b[A\n",
            " 60% 6300/10570 [00:11<00:08, 532.27it/s]\u001b[A\n",
            " 60% 6358/10570 [00:11<00:07, 545.52it/s]\u001b[A\n",
            " 61% 6413/10570 [00:11<00:07, 531.27it/s]\u001b[A\n",
            " 61% 6472/10570 [00:11<00:07, 546.32it/s]\u001b[A\n",
            " 62% 6528/10570 [00:12<00:07, 540.06it/s]\u001b[A\n",
            " 62% 6585/10570 [00:12<00:07, 545.84it/s]\u001b[A\n",
            " 63% 6640/10570 [00:12<00:07, 535.88it/s]\u001b[A\n",
            " 63% 6694/10570 [00:12<00:07, 533.54it/s]\u001b[A\n",
            " 64% 6750/10570 [00:12<00:07, 538.87it/s]\u001b[A\n",
            " 64% 6809/10570 [00:12<00:06, 549.97it/s]\u001b[A\n",
            " 65% 6865/10570 [00:12<00:06, 543.60it/s]\u001b[A\n",
            " 65% 6922/10570 [00:12<00:06, 549.51it/s]\u001b[A\n",
            " 66% 6978/10570 [00:12<00:06, 551.43it/s]\u001b[A\n",
            " 67% 7037/10570 [00:12<00:06, 560.19it/s]\u001b[A\n",
            " 67% 7094/10570 [00:13<00:06, 555.43it/s]\u001b[A\n",
            " 68% 7150/10570 [00:13<00:06, 554.10it/s]\u001b[A\n",
            " 68% 7206/10570 [00:13<00:06, 552.78it/s]\u001b[A\n",
            " 69% 7262/10570 [00:13<00:06, 546.29it/s]\u001b[A\n",
            " 69% 7321/10570 [00:13<00:05, 555.36it/s]\u001b[A\n",
            " 70% 7377/10570 [00:13<00:06, 527.43it/s]\u001b[A\n",
            " 70% 7434/10570 [00:13<00:05, 538.14it/s]\u001b[A\n",
            " 71% 7489/10570 [00:13<00:05, 540.37it/s]\u001b[A\n",
            " 71% 7546/10570 [00:13<00:05, 547.11it/s]\u001b[A\n",
            " 72% 7601/10570 [00:13<00:05, 544.37it/s]\u001b[A\n",
            " 72% 7656/10570 [00:14<00:05, 533.18it/s]\u001b[A\n",
            " 73% 7712/10570 [00:14<00:05, 540.34it/s]\u001b[A\n",
            " 73% 7767/10570 [00:14<00:05, 533.13it/s]\u001b[A\n",
            " 74% 7822/10570 [00:14<00:05, 535.68it/s]\u001b[A\n",
            " 75% 7880/10570 [00:14<00:04, 546.62it/s]\u001b[A\n",
            " 75% 7937/10570 [00:14<00:04, 553.37it/s]\u001b[A\n",
            " 76% 7993/10570 [00:14<00:04, 552.20it/s]\u001b[A\n",
            " 76% 8049/10570 [00:14<00:04, 530.27it/s]\u001b[A\n",
            " 77% 8106/10570 [00:14<00:04, 541.37it/s]\u001b[A\n",
            " 77% 8161/10570 [00:15<00:04, 539.34it/s]\u001b[A\n",
            " 78% 8216/10570 [00:15<00:04, 515.16it/s]\u001b[A\n",
            " 78% 8268/10570 [00:15<00:04, 513.37it/s]\u001b[A\n",
            " 79% 8320/10570 [00:15<00:04, 506.23it/s]\u001b[A\n",
            " 79% 8375/10570 [00:15<00:04, 516.05it/s]\u001b[A\n",
            " 80% 8434/10570 [00:15<00:03, 535.93it/s]\u001b[A\n",
            " 80% 8489/10570 [00:15<00:03, 539.22it/s]\u001b[A\n",
            " 81% 8549/10570 [00:15<00:03, 553.62it/s]\u001b[A\n",
            " 81% 8605/10570 [00:15<00:03, 552.23it/s]\u001b[A\n",
            " 82% 8661/10570 [00:15<00:03, 552.86it/s]\u001b[A\n",
            " 82% 8717/10570 [00:16<00:03, 550.77it/s]\u001b[A\n",
            " 83% 8773/10570 [00:16<00:03, 537.13it/s]\u001b[A\n",
            " 84% 8829/10570 [00:16<00:03, 542.12it/s]\u001b[A\n",
            " 84% 8887/10570 [00:16<00:03, 551.64it/s]\u001b[A\n",
            " 85% 8943/10570 [00:16<00:02, 547.46it/s]\u001b[A\n",
            " 85% 9000/10570 [00:16<00:02, 551.83it/s]\u001b[A\n",
            " 86% 9056/10570 [00:16<00:02, 548.71it/s]\u001b[A\n",
            " 86% 9111/10570 [00:16<00:02, 547.97it/s]\u001b[A\n",
            " 87% 9166/10570 [00:16<00:02, 546.71it/s]\u001b[A\n",
            " 87% 9224/10570 [00:16<00:02, 554.82it/s]\u001b[A\n",
            " 88% 9281/10570 [00:17<00:02, 557.53it/s]\u001b[A\n",
            " 88% 9337/10570 [00:17<00:02, 550.77it/s]\u001b[A\n",
            " 89% 9395/10570 [00:17<00:02, 558.94it/s]\u001b[A\n",
            " 89% 9451/10570 [00:17<00:02, 554.86it/s]\u001b[A\n",
            " 90% 9507/10570 [00:17<00:01, 553.75it/s]\u001b[A\n",
            " 90% 9565/10570 [00:17<00:01, 558.32it/s]\u001b[A\n",
            " 91% 9621/10570 [00:17<00:01, 557.96it/s]\u001b[A\n",
            " 92% 9677/10570 [00:17<00:01, 554.63it/s]\u001b[A\n",
            " 92% 9737/10570 [00:17<00:01, 566.58it/s]\u001b[A\n",
            " 93% 9794/10570 [00:18<00:01, 541.63it/s]\u001b[A\n",
            " 93% 9849/10570 [00:18<00:01, 537.02it/s]\u001b[A\n",
            " 94% 9904/10570 [00:18<00:01, 537.85it/s]\u001b[A\n",
            " 94% 9962/10570 [00:18<00:01, 549.55it/s]\u001b[A\n",
            " 95% 10018/10570 [00:18<00:01, 536.31it/s]\u001b[A\n",
            " 95% 10075/10570 [00:18<00:00, 544.02it/s]\u001b[A\n",
            " 96% 10131/10570 [00:18<00:00, 546.66it/s]\u001b[A\n",
            " 96% 10186/10570 [00:18<00:00, 540.92it/s]\u001b[A\n",
            " 97% 10243/10570 [00:18<00:00, 549.25it/s]\u001b[A\n",
            " 97% 10300/10570 [00:18<00:00, 552.45it/s]\u001b[A\n",
            " 98% 10358/10570 [00:19<00:00, 559.47it/s]\u001b[A\n",
            " 99% 10415/10570 [00:19<00:00, 543.03it/s]\u001b[A\n",
            " 99% 10470/10570 [00:19<00:00, 531.70it/s]\u001b[A\n",
            "100% 10570/10570 [00:19<00:00, 543.86it/s]\n",
            "12/16/2020 17:07:02 - INFO - utils_qa -   Saving predictions to abhilash1910/distilbert-squadv1/predictions.json.\n",
            "12/16/2020 17:07:02 - INFO - utils_qa -   Saving nbest_preds to abhilash1910/distilbert-squadv1/nbest_predictions.json.\n",
            "100% 1353/1353 [03:01<00:00,  7.45it/s]\n",
            "12/16/2020 17:07:07 - INFO - __main__ -   ***** Eval results *****\n",
            "12/16/2020 17:07:07 - INFO - __main__ -     exact_match = 72.87606433301798\n",
            "12/16/2020 17:07:07 - INFO - __main__ -     f1 = 82.01051487926736\n",
            "12/16/2020 17:07:07 - INFO - __main__ -     epoch = 0.7501014198782961\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3eLI-Vvem1V",
        "outputId": "8af8fe51-a7ce-49ec-8e37-da5c6dcec6af"
      },
      "source": [
        "import glob\r\n",
        "\r\n",
        "files=glob.glob('./abhilash1910/distilbert-squadv1/*')\r\n",
        "files"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./abhilash1910/distilbert-squadv1/eval_results.txt',\n",
              " './abhilash1910/distilbert-squadv1/vocab.txt',\n",
              " './abhilash1910/distilbert-squadv1/checkpoint-500',\n",
              " './abhilash1910/distilbert-squadv1/checkpoint-4500',\n",
              " './abhilash1910/distilbert-squadv1/pytorch_model.bin',\n",
              " './abhilash1910/distilbert-squadv1/checkpoint-1500',\n",
              " './abhilash1910/distilbert-squadv1/config.json',\n",
              " './abhilash1910/distilbert-squadv1/checkpoint-2000',\n",
              " './abhilash1910/distilbert-squadv1/tokenizer_config.json',\n",
              " './abhilash1910/distilbert-squadv1/training_args.bin',\n",
              " './abhilash1910/distilbert-squadv1/checkpoint-3500',\n",
              " './abhilash1910/distilbert-squadv1/checkpoint-5500',\n",
              " './abhilash1910/distilbert-squadv1/special_tokens_map.json',\n",
              " './abhilash1910/distilbert-squadv1/checkpoint-2500',\n",
              " './abhilash1910/distilbert-squadv1/checkpoint-4000',\n",
              " './abhilash1910/distilbert-squadv1/checkpoint-3000',\n",
              " './abhilash1910/distilbert-squadv1/checkpoint-5000',\n",
              " './abhilash1910/distilbert-squadv1/predictions.json',\n",
              " './abhilash1910/distilbert-squadv1/nbest_predictions.json',\n",
              " './abhilash1910/distilbert-squadv1/checkpoint-1000']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IejL04TyEb1h"
      },
      "source": [
        "## Testing the Model\r\n",
        "\r\n",
        "We first test the trained model by using some example and [NLP pipeline](https://huggingface.co/transformers/main_classes/pipelines.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7Au7fxxrSLb",
        "outputId": "33f5808d-79cd-4fe3-a2cb-efe0dfac30e7"
      },
      "source": [
        "from transformers import AutoModelForQuestionAnswering,AutoTokenizer,pipeline\r\n",
        "nlp_QA=pipeline('question-answering',model='./abhilash1910/distilbert-squadv1',tokenizer='./abhilash1910/distilbert-squadv1')\r\n",
        "QA_inp={\r\n",
        "    'question': 'What is the fund price of Huggingface in NYSE?',\r\n",
        "    'context': 'Huggingface Co. has a total fund price of $19.6 million dollars'\r\n",
        "}\r\n",
        "result=nlp_QA(QA_inp)\r\n",
        "result"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': '$19.6 million dollars',\n",
              " 'end': 63,\n",
              " 'score': 0.8255521655082703,\n",
              " 'start': 42}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT6pTPC-Eu1N"
      },
      "source": [
        "## Uploading Model to Huggingface\r\n",
        "\r\n",
        "This [webpage](https://huggingface.co/transformers/model_sharing.html) contains the details for uploading models.We will be using the Colab Uploads for our use case. \r\n",
        "\r\n",
        "The first step involves locally authenticating with Huggingface CLI and saving our session token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c74fmm-htRJ1"
      },
      "source": [
        "!transformers-cli login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjTmWfdkFFc_"
      },
      "source": [
        "## Creating a Repo in Huggingface Models\r\n",
        "\r\n",
        "In this case, we first create our own repository in the [Huggingface Models Repository](https://huggingface.co)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mKuPogftswM"
      },
      "source": [
        "!transformers-cli repo create 'distilbert-squadv1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYFlIFP_FVo9"
      },
      "source": [
        "## Cloning the Newly Created Repository to our Local Notebook\r\n",
        "\r\n",
        "We can clone the repository as we will be uploading this after commiting all the files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkJ6aZzJuRLn",
        "outputId": "9dd5bab9-a30d-49c4-b4d7-1c291eb87d72"
      },
      "source": [
        "!git clone https://huggingface.co/abhilash1910/distilbert-squadv1"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'distilbert-squadv1'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 3 (delta 0), reused 0 (delta 0)\u001b[K\n",
            "Unpacking objects: 100% (3/3), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKsMAIhyuaal"
      },
      "source": [
        "!cd distilbert-squadv1\r\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "solKvuLMFiid"
      },
      "source": [
        "## Installing Git Lfs\r\n",
        "\r\n",
        "Since we have to send a large pytorch/tensorflow model file containing weights (either in binary or hdf5), we have to install Git LFS. In command line this is as simple as :\r\n",
        "\r\n",
        "```bash\r\n",
        "\r\n",
        "git lfs install\r\n",
        "\r\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgR-qsJ6vDSC",
        "outputId": "a3149813-fa2f-4910-e0d0-a768c1619cd3"
      },
      "source": [
        "!sudo apt-get install git-lfs"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  git-lfs\n",
            "0 upgraded, 1 newly installed, 0 to remove and 14 not upgraded.\n",
            "Need to get 2,129 kB of archives.\n",
            "After this operation, 7,662 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 git-lfs amd64 2.3.4-1 [2,129 kB]\n",
            "Fetched 2,129 kB in 1s (2,846 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package git-lfs.\n",
            "(Reading database ... 144865 files and directories currently installed.)\n",
            "Preparing to unpack .../git-lfs_2.3.4-1_amd64.deb ...\n",
            "Unpacking git-lfs (2.3.4-1) ...\n",
            "Setting up git-lfs (2.3.4-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRVDVUIMwsXA",
        "outputId": "a2be147f-09c3-4663-bbc3-6253d01d0768"
      },
      "source": [
        "!dir"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "abhilash1910\t    __pycache__  runs\t      trainer_qa.py  utils_qa.py\n",
            "distilbert-squadv1  run_qa.py\t sample_data  transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP20URumw7L2"
      },
      "source": [
        "%cd abhilash1910/distilbert-squadv1/distilbert-squadv1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvkcxWBh_63l",
        "outputId": "ed26df0e-6b4d-4551-c693-8f7574f8f2aa"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/abhilash1910/distilbert-squadv1/distilbert-squadv1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTwZycdAF34D"
      },
      "source": [
        "## Navigate to the Directory containing the files\r\n",
        "\r\n",
        "Make sure the following files are present:\r\n",
        "\r\n",
        "- a config.json file, which saves the configuration of your model ;\r\n",
        "\r\n",
        "- a pytorch_model.bin file, which is the PyTorch checkpoint (unless you can’t have it for some reason) ;\r\n",
        "\r\n",
        "- a tf_model.h5 file, which is the TensorFlow checkpoint (unless you can’t have it for some reason) ;\r\n",
        "\r\n",
        "- a special_tokens_map.json, which is part of your tokenizer save;\r\n",
        "\r\n",
        "- a tokenizer_config.json, which is part of your tokenizer save;\r\n",
        "\r\n",
        "- files named vocab.json, vocab.txt, merges.txt, or similar, which contain the vocabulary of your tokenizer, part of your tokenizer save;\r\n",
        "\r\n",
        "- maybe a added_tokens.json, which is part of your tokenizer save.\r\n",
        "\r\n",
        "Additional files can be removed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6fvBvDUxJ5W",
        "outputId": "86392374-88ea-4aba-99a7-83adddfccc74"
      },
      "source": [
        "import glob\r\n",
        "\r\n",
        "files=glob.glob('./abhilash1910/distilbert-squadv1/*')\r\n",
        "files"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./abhilash1910/distilbert-squadv1/eval_results.txt',\n",
              " './abhilash1910/distilbert-squadv1/vocab.txt',\n",
              " './abhilash1910/distilbert-squadv1/checkpoint-500',\n",
              " './abhilash1910/distilbert-squadv1/checkpoint-4500',\n",
              " './abhilash1910/distilbert-squadv1/pytorch_model.bin',\n",
              " './abhilash1910/distilbert-squadv1/checkpoint-1500',\n",
              " './abhilash1910/distilbert-squadv1/config.json',\n",
              " './abhilash1910/distilbert-squadv1/checkpoint-2000',\n",
              " './abhilash1910/distilbert-squadv1/tokenizer_config.json',\n",
              " './abhilash1910/distilbert-squadv1/training_args.bin',\n",
              " './abhilash1910/distilbert-squadv1/checkpoint-3500',\n",
              " './abhilash1910/distilbert-squadv1/checkpoint-5500',\n",
              " './abhilash1910/distilbert-squadv1/special_tokens_map.json',\n",
              " './abhilash1910/distilbert-squadv1/checkpoint-2500',\n",
              " './abhilash1910/distilbert-squadv1/checkpoint-4000',\n",
              " './abhilash1910/distilbert-squadv1/checkpoint-3000',\n",
              " './abhilash1910/distilbert-squadv1/checkpoint-5000',\n",
              " './abhilash1910/distilbert-squadv1/predictions.json',\n",
              " './abhilash1910/distilbert-squadv1/nbest_predictions.json',\n",
              " './abhilash1910/distilbert-squadv1/checkpoint-1000']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "virolQ5kxP1q"
      },
      "source": [
        "!git add *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FOftoB0GM-e"
      },
      "source": [
        "## Set Configurations for User Name & Email\r\n",
        "\r\n",
        "We have to provie the configurations for our huggingface email and user name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INrmfpyzEQG7"
      },
      "source": [
        "!git config --global user.email \"debabhi1396@gmail.com\"\r\n",
        "!git config --global user.name \"abhilash1910\""
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83ye2fUsGdDY"
      },
      "source": [
        "## Commiting the Changes\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23Grsw_5D6k7",
        "outputId": "088a6866-ec6d-4121-eadd-d6193e03073e"
      },
      "source": [
        "!git commit -m \"Initial Commit\""
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[main a514dfb] Initial Commit\n",
            " 108 files changed, 1648941 insertions(+)\n",
            " create mode 100644 distilbert-squadv1/checkpoint-1000/config.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-1000/optimizer.pt\n",
            " create mode 100644 distilbert-squadv1/checkpoint-1000/pytorch_model.bin\n",
            " create mode 100644 distilbert-squadv1/checkpoint-1000/scheduler.pt\n",
            " create mode 100644 distilbert-squadv1/checkpoint-1000/special_tokens_map.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-1000/tokenizer_config.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-1000/trainer_state.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-1000/training_args.bin\n",
            " create mode 100644 distilbert-squadv1/checkpoint-1000/vocab.txt\n",
            " create mode 100644 distilbert-squadv1/checkpoint-1500/config.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-1500/optimizer.pt\n",
            " create mode 100644 distilbert-squadv1/checkpoint-1500/pytorch_model.bin\n",
            " create mode 100644 distilbert-squadv1/checkpoint-1500/scheduler.pt\n",
            " create mode 100644 distilbert-squadv1/checkpoint-1500/special_tokens_map.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-1500/tokenizer_config.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-1500/trainer_state.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-1500/training_args.bin\n",
            " create mode 100644 distilbert-squadv1/checkpoint-1500/vocab.txt\n",
            " create mode 100644 distilbert-squadv1/checkpoint-2000/config.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-2000/optimizer.pt\n",
            " create mode 100644 distilbert-squadv1/checkpoint-2000/pytorch_model.bin\n",
            " create mode 100644 distilbert-squadv1/checkpoint-2000/scheduler.pt\n",
            " create mode 100644 distilbert-squadv1/checkpoint-2000/special_tokens_map.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-2000/tokenizer_config.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-2000/trainer_state.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-2000/training_args.bin\n",
            " create mode 100644 distilbert-squadv1/checkpoint-2000/vocab.txt\n",
            " create mode 100644 distilbert-squadv1/checkpoint-2500/config.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-2500/optimizer.pt\n",
            " create mode 100644 distilbert-squadv1/checkpoint-2500/pytorch_model.bin\n",
            " create mode 100644 distilbert-squadv1/checkpoint-2500/scheduler.pt\n",
            " create mode 100644 distilbert-squadv1/checkpoint-2500/special_tokens_map.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-2500/tokenizer_config.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-2500/trainer_state.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-2500/training_args.bin\n",
            " create mode 100644 distilbert-squadv1/checkpoint-2500/vocab.txt\n",
            " create mode 100644 distilbert-squadv1/checkpoint-3000/config.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-3000/optimizer.pt\n",
            " create mode 100644 distilbert-squadv1/checkpoint-3000/pytorch_model.bin\n",
            " create mode 100644 distilbert-squadv1/checkpoint-3000/scheduler.pt\n",
            " create mode 100644 distilbert-squadv1/checkpoint-3000/special_tokens_map.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-3000/tokenizer_config.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-3000/trainer_state.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-3000/training_args.bin\n",
            " create mode 100644 distilbert-squadv1/checkpoint-3000/vocab.txt\n",
            " create mode 100644 distilbert-squadv1/checkpoint-3500/config.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-3500/optimizer.pt\n",
            " create mode 100644 distilbert-squadv1/checkpoint-3500/pytorch_model.bin\n",
            " create mode 100644 distilbert-squadv1/checkpoint-3500/scheduler.pt\n",
            " create mode 100644 distilbert-squadv1/checkpoint-3500/special_tokens_map.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-3500/tokenizer_config.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-3500/trainer_state.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-3500/training_args.bin\n",
            " create mode 100644 distilbert-squadv1/checkpoint-3500/vocab.txt\n",
            " create mode 100644 distilbert-squadv1/checkpoint-4000/config.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-4000/optimizer.pt\n",
            " create mode 100644 distilbert-squadv1/checkpoint-4000/pytorch_model.bin\n",
            " create mode 100644 distilbert-squadv1/checkpoint-4000/scheduler.pt\n",
            " create mode 100644 distilbert-squadv1/checkpoint-4000/special_tokens_map.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-4000/tokenizer_config.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-4000/trainer_state.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-4000/training_args.bin\n",
            " create mode 100644 distilbert-squadv1/checkpoint-4000/vocab.txt\n",
            " create mode 100644 distilbert-squadv1/checkpoint-4500/config.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-4500/optimizer.pt\n",
            " create mode 100644 distilbert-squadv1/checkpoint-4500/pytorch_model.bin\n",
            " create mode 100644 distilbert-squadv1/checkpoint-4500/scheduler.pt\n",
            " create mode 100644 distilbert-squadv1/checkpoint-4500/special_tokens_map.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-4500/tokenizer_config.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-4500/trainer_state.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-4500/training_args.bin\n",
            " create mode 100644 distilbert-squadv1/checkpoint-4500/vocab.txt\n",
            " create mode 100644 distilbert-squadv1/checkpoint-500/config.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-500/optimizer.pt\n",
            " create mode 100644 distilbert-squadv1/checkpoint-500/pytorch_model.bin\n",
            " create mode 100644 distilbert-squadv1/checkpoint-500/scheduler.pt\n",
            " create mode 100644 distilbert-squadv1/checkpoint-500/special_tokens_map.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-500/tokenizer_config.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-500/trainer_state.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-500/training_args.bin\n",
            " create mode 100644 distilbert-squadv1/checkpoint-500/vocab.txt\n",
            " create mode 100644 distilbert-squadv1/checkpoint-5000/config.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-5000/optimizer.pt\n",
            " create mode 100644 distilbert-squadv1/checkpoint-5000/pytorch_model.bin\n",
            " create mode 100644 distilbert-squadv1/checkpoint-5000/scheduler.pt\n",
            " create mode 100644 distilbert-squadv1/checkpoint-5000/special_tokens_map.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-5000/tokenizer_config.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-5000/trainer_state.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-5000/training_args.bin\n",
            " create mode 100644 distilbert-squadv1/checkpoint-5000/vocab.txt\n",
            " create mode 100644 distilbert-squadv1/checkpoint-5500/config.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-5500/optimizer.pt\n",
            " create mode 100644 distilbert-squadv1/checkpoint-5500/pytorch_model.bin\n",
            " create mode 100644 distilbert-squadv1/checkpoint-5500/scheduler.pt\n",
            " create mode 100644 distilbert-squadv1/checkpoint-5500/special_tokens_map.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-5500/tokenizer_config.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-5500/trainer_state.json\n",
            " create mode 100644 distilbert-squadv1/checkpoint-5500/training_args.bin\n",
            " create mode 100644 distilbert-squadv1/checkpoint-5500/vocab.txt\n",
            " create mode 100644 distilbert-squadv1/config.json\n",
            " create mode 100644 distilbert-squadv1/eval_results.txt\n",
            " create mode 100644 distilbert-squadv1/nbest_predictions.json\n",
            " create mode 100644 distilbert-squadv1/predictions.json\n",
            " create mode 100644 distilbert-squadv1/pytorch_model.bin\n",
            " create mode 100644 distilbert-squadv1/special_tokens_map.json\n",
            " create mode 100644 distilbert-squadv1/tokenizer_config.json\n",
            " create mode 100644 distilbert-squadv1/training_args.bin\n",
            " create mode 100644 distilbert-squadv1/vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mR556RHGjss"
      },
      "source": [
        "## Pushing the Model Files to Huggingface Repository\r\n",
        "\r\n",
        "In this case, uploading from colab has some difficulties related to Github SSH authentication. The best way to upload is to include username and password in the format:\r\n",
        "\r\n",
        "```bash\r\n",
        "https://username:password@huggingface.co/<username/model-name>\r\n",
        "```\r\n",
        "\r\n",
        "This allows the model files to get uploaded to the repositoy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1H_HLHn9EeRr"
      },
      "source": [
        "!git push https://username:password@huggingface.co/abhilash1910/distilbert-squadv1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJ6z1DAgHD2t"
      },
      "source": [
        "## Conclusion\r\n",
        "\r\n",
        "This Notebook is curated from best practises to follow while training a model on QA downstream tasks within Google Colab.  "
      ]
    }
  ]
}